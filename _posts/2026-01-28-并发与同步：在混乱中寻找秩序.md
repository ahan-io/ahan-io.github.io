---
layout: post
title: "并发与同步：在混乱中寻找秩序"
author: "Ahan"
date: 2026-01-28 00:00:00
header-img: "img/post-bg-2015.jpg"
header-style: text
catalog:      true
tags:
    - Architecture
    - Stability
    - 分布式
---
> 几乎所有分布式锁都是错的。
> 

并发编程一直被认为是工程实践中最容易“出意外”的领域。即使是拥有十年以上经验的工程师，也常常在这里踩坑。

其困难的根源在于两个核心问题：

1. **如何确保关键资源只能被串行修改；**
2. **当系统跨越多台机器时，如何在分布式环境中仍然满足第一条。**

同步机制的目标，正是为了解决这些问题——让对同一资源的修改**在逻辑上有序、在结果上正确**，避免出现并发写入带来的不确定行为。

从单机到分布式，常见的同步方式包括：

- **内存锁（Mutex / Semaphore）**：最基础的同步原语，用于线程级别的互斥控制。
- **其它机制**：例如本地文件锁、数据库锁等场景化手段。
- **分布式锁（Distributed Lock）**：用于多节点场景，通过外部系统（如 Redis、Zookeeper）协调访问顺序。
- **CAS（Compare-And-Swap）**：通过原子指令在用户态实现轻量的并发控制。
- **分布式一致性协议（Consensus Protocol）**：当系统规模进一步扩大时，用于保障多个节点之间状态的一致性。

这些机制共同构成了并发控制的基础。从中我们可以看到一个明显的演进路径：
当系统还处在单机阶段时，内存锁几乎可以解决所有同步问题；
而当系统变得更复杂，跨越多核、多进程、乃至多节点时，锁的作用范围逐渐失效，我们必须寻找新的同步手段。

接下来，我们从最基础的“并发锁”开始，理解它如何在单机范围内建立起最初的秩序。

# 内存锁：从硬件开始的秩序

在现代多核处理器的世界里，所有的计算几乎都在并发中进行。线程像是工厂中的工人，共享着同一片内存车间。当他们同时读写同一个变量时，混乱就可能发生——这就是“竞态条件（race condition）”。为了让这些工人不会互相踩脚，我们需要“锁”（lock），而“内存锁”则是实现这一切的底层机制。

从本质上说，**内存锁是一种CPU层面的同步原语**。它通过硬件指令或总线协议，保证特定内存操作的**原子性（atomicity）与可见性（visibility）**，从而防止多个核心同时修改同一块数据导致状态错乱。

## 总线锁与缓存锁

在早期的单核或双核架构中，CPU通常通过**总线锁（Bus Lock）**来实现内存同步。当执行一条带有 `LOCK` 前缀的指令（如 `LOCK XCHG`）时，处理器会独占系统总线，禁止其他CPU访问内存。这种方式简单但代价高昂：在锁持有期间，所有核都被迫等待。

后来，随着缓存一致性协议（MESI）的出现，出现了更高效的**缓存锁（Cache Lock）**机制。CPU在本地缓存行中锁定目标变量，而不是锁整个总线。这样，只要数据仍在该缓存行内，就能避免跨总线竞争，大幅提升性能。这是现代x86架构中常见的实现方式。

## 指令级锁与内存屏障

在指令层面，常见的原子操作包括 `XCHG`、`CMPXCHG`、`FETCH_AND_ADD` 等。这些操作保证一个读-改-写序列不会被其他CPU中断。

但光有原子性还不够。多核处理器会对指令进行乱序执行（Out-of-Order Execution），导致内存可见性问题。例如，一个线程写入变量A后写入B，另一个线程可能先看到B被修改，却还没看到A更新。

为此，硬件和编译器引入了**内存屏障（Memory Barrier / Fence）**机制，分为以下几类：

- **Load Barrier（读屏障）**：保证之前的读操作完成后，后续读才能进行。
- **Store Barrier（写屏障）**：确保之前的写操作在后续写之前被提交。
- **Full Fence（全屏障）**：同时限制读写重排。

在C/C++、Go或Rust等语言中，这些机制通过关键字如 `atomic.Store`, `memory_order_seq_cst`, 或 `sync/atomic` 包来实现。

## 语言层锁：从机制到语义

| **锁类型** | **适用范围** | **备注** |
| --- | --- | --- |
| 自旋锁（Spin Lock） | 自旋锁是一种轻量级锁，线程在等待锁时不会睡眠，而是在CPU上不断循环检查锁状态。适用于锁持有时间极短的场景。

例如加锁后，仅仅是修改一个内存中的变量（速度超级快）。 | 由于自旋锁的特性，不允许在自旋锁挂锁期间做其它耗时的操作（IO），例如打印日志，收发网络消息。 |
| 互斥锁（Mutex） | 等待期间将线程挂起，避免浪费CPU时间，适合长耗时的临界区。 |  |
| 读写锁 | 进一步的优化出现于**读写锁（Read-Write Lock）**。在读多写少的场景下，多个线程可以同时持有读锁，而写锁独占资源。 | 大部分对性能要求不是非常严苛的业务代码，读写锁是更常见的选择。 |

这些锁的实现都依赖底层的原子指令：比如 `Test-and-Set`、`Compare-and-Swap (CAS)`。

## 无锁结构：让竞争更细粒度

在多线程世界里，传统的锁（Lock）像红绿灯，能避免碰撞，却也带来了等待和堵塞。**无锁编程（Lock-Free Programming）**的核心思想，就是让线程在不使用互斥锁的情况下安全地访问共享数据，从而提升并发性能、降低延迟风险。

无锁并不是“没有同步”，而是**通过原子操作（Atomic Operation）来实现同步**。这些操作由CPU直接支持，例如 `Compare-And-Swap (CAS)`、`Fetch-And-Add`、`Load-Link/Store-Conditional` 等。它们保证一次读-改-写过程的原子性：

- 当多个线程同时修改同一变量时，只有一个能成功，其余线程会检测到失败并重试。
    
    这种方式避免了线程阻塞，因此系统整体吞吐量更高，也不会因某个线程挂起而导致“锁被永远持有”的问题。
    

无锁结构最常见的应用，是**无锁队列、无锁栈、无锁引用计数器**等。这类结构通常依赖 CAS 循环：线程先读取当前值，计算新值，再尝试用 CAS 更新；如果更新失败，说明其他线程抢先修改了数据，于是重试。尽管这会带来一定的忙等（busy-wait）开销，但相比互斥锁的上下文切换代价要轻得多。

当然，无锁编程并非银弹。它的实现复杂且容易出错。比如“ABA 问题”——一个变量从 A 变成 B 又变回 A，CAS 比较时误以为它没变；或者在回收对象时，另一个线程仍在访问它，导致悬挂指针。这类问题通常需要配合 **内存屏障（Memory Fence）** 或 **Hazard Pointer**、**Epoch-based Reclamation** 等机制来避免。

**锁是安全但笨重的，CAS 是危险但高效的。**

无锁编程追求的是在正确性与性能之间的极限平衡。它不是为了炫技，而是在极端性能要求下的一种“工程必要性”——当每一纳秒都关乎稳定性与响应速度时，锁的成本就变成了必须剔除的负担。 

 无锁机制目前已经被广泛应用，例如：

- 虚拟化 I/O 框架（如 SPDK / DPDK）：在高性能虚拟化或网络场景中（如 NVMe-over-Fabrics、用户态网卡驱动），传统锁会极大拖慢 I/O 吞吐。SPDK、DPDK 通过 无锁队列（Lock-Free Ring Buffer） 实现多核间数据交换，用 CAS 和内存屏障取代互斥锁，使得数据在 NUMA 架构中高效传递。
- Virtio / Vhost 驱动体系：在虚拟机与宿主机的数据交换中，Virtio 通过设计一种单向写入的环形队列（Virtqueue）实现无锁通信。Guest 只负责写入请求的 available ring，而 Host 只更新 used ring，双方用独立的索引指针和内存屏障协调进度，无需加锁即可安全并行。这种结构性分工避免了多核下的竞争开销，并通过多队列（Multi-Queue）和批量通知（Batch Notification）进一步降低同步成本。Virtio-net、Virtio-blk 等驱动几乎完全依赖这一无锁机制，实现了接近裸机的 I/O 性能。
- 多生产者多消费者队列（MPMC Queue）：这是无锁编程的经典结构之一，用于线程间消息传递、任务分发等场景。典型实现如 Michael & Scott Queue，通过 CAS 操作维护链表头尾指针，避免全局锁竞争。
- Linux RCU（Read-Copy-Update）：内核中最著名的无锁同步机制之一。它允许读者无锁地访问共享数据，而写者通过复制和延迟回收实现一致性。RCU 广泛用于调度器、网络子系统、文件系统等模块，使得大量读操作几乎零开销，从而在多核环境中保持线性扩展性。
- 高性能缓存与数据库系统（如 Redis、RocksDB）：在并发读多于写的场景中，很多组件采用 Lock-Free Skip List 或 Lock-Free LSM Tree 来降低写锁争用。写入采用版本化（Versioned Write）或原子替换结构（Atomic Pointer Swap）策略，让读线程不必阻塞，从而大幅提升 QPS 与延迟表现。

## 锁的权衡

锁是并发世界的“摩擦力”。它带来秩序，也带来性能损耗。理想的设计不是“没有锁”，而是“锁的粒度刚刚好”：

- 锁得太粗，会导致性能瓶颈；
- 锁得太细，则可能出现死锁或维护复杂度飙升。

工程的艺术在于平衡。你既要理解指令级的 `LOCK CMPXCHG`，也要知道在高层服务架构中，一个“互斥”往往意味着可伸缩性的瓶颈。

## 如何尽可能避免 data race

锁的正确使用，是编程中最容易低估、却最容易出错的领域之一。几乎所有工程师在职业生涯的前五年里，都会不同程度地在并发正确性上栽过跟头。**Data Race（数据竞争）** 往往不是因为粗心，而是因为人类难以同时在脑中维护多个线程的执行路径。

因此，在团队代码评审（Code Review）中，并发相关的代码应被视为**高风险区域**。任何共享状态、可变引用、延迟执行的闭包，都应被重点审查。

现代编程语言开始提供对数据竞争的检测支持。例如 **Go** 内置了强大的数据竞争检测器，无需额外依赖，只需在编译、运行或测试时加上参数 `-race` 即可自动检测潜在问题：

```bash
go run -race main.go
go test -race ./...
```

需要注意的是，**启用 `-race` 会显著降低程序性能（通常慢 5~10 倍）并占用更多内存**，因此它适合用于开发和测试阶段，而非生产环境。

当然，如果条件允许，引入在编译期即可**彻底消除数据竞争**的语言（如 **Rust**）是更根本的解决方案。Rust 的类型系统与所有权模型在编译阶段就能捕获潜在的并发访问问题——也就是说，它让“Race Condition”在语义上不可能存在。

这并不是理论上的完美。我们曾在一套使用 C 编写的分布式文件系统中，经历过一次令人抓狂的排查：一个几乎无法复现的**微量内存泄漏**，持续数周后才被确认是由一次 **data race 引起的内存踩踏**。由于引用计数在竞争条件下被错误修改，系统的内存被缓慢泄漏。若使用 Rust，这类问题将在编译期被直接拒绝。

**结论很简单：**

> 当数据竞争的代价不可承受时，让编译器替你做守门员。
> 
> 
> 在并发正确性这件事上，机器确实比人更靠谱。
> 

# 其它类型的锁

在进入分布式锁之前，我们先看一些“更低层”的同步手段。

并不是所有并发问题都要靠分布式方案解决，很多时候，一个**简单的本地锁**或**数据库内的事务控制**就足够。

## **本地文件锁**

当系统的运行范围局限在单机内（例如一台服务器上的多个进程），可以借助文件系统自带的锁机制来实现互斥。

常见的做法有两种：

- **`flock`（Advisory Lock）**：是一种“协作式锁”，只有进程遵守约定、主动调用解锁，才能真正发挥作用。它不会强制阻止文件访问。
- **`fcntl`（Record Locking）**：支持对文件的部分区域加锁，适合多进程共享同一文件、但操作不同片段的场景。

文件锁通常依赖操作系统的文件描述符（File Descriptor）实现，当持锁进程崩溃或退出时，锁会自动释放。

因此它非常适合用在**单机多进程间的轻量同步**中，比如：

- 防止多个定时任务同时写入同一日志；
- 确保某个脚本在同一时刻只运行一个实例；
- 在简单的本地缓存中维持原子写操作。

文件锁的局限在于：它的作用范围只在**文件系统的单一挂载点**之内，无法跨机器。

一旦应用进入分布式环境，文件锁就无法保证全局互斥。

## **数据库锁**

当数据存储集中在数据库中时，数据库自身就成为最自然的同步边界。

与本地锁相比，数据库提供了更丰富的并发控制语义：

它既能保证事务的原子性（Atomicity），也能保证读写顺序的一致性（Isolation）。

在多数业务系统中，对象的状态往往保存在数据库里。

这意味着我们可以直接利用数据库的事务特性来实现“逻辑锁”，而不必额外引入外部锁服务。

几种常见的实践方式如下：

1. **事务 + `SELECT ... FOR UPDATE`**
    
    将查询与修改放入同一个事务中，并通过 `SELECT ... FOR UPDATE` 显式加锁。
    
    这会在数据库层面为选定的记录加上排他锁，其他事务在提交前无法修改同一行。
    
    示例：
    
    ```sql
    BEGIN;
    SELECT balance FROM accounts WHERE id = 42 FOR UPDATE;
    UPDATE accounts SET balance = balance - 100 WHERE id = 42;
    COMMIT;
    
    ```
    
    这种方式保证了强一致性，适用于**短事务、低延迟的并发修改**。
    
2. **手动 CAS（Compare-And-Swap）**
    
    对于不方便显式加锁的场景，可以用一种“乐观锁”的思路：
    
    让更新语句带上期望版本或旧值：
    
    ```sql
    UPDATE inventory
    SET stock = stock - 1
    WHERE item_id = 1001 AND stock = 10;
    
    ```
    
    如果 `stock` 已被其他事务修改，这条更新语句不会生效。
    
    应用层可以通过返回的行数（`rows_affected`）判断是否成功，并决定是否重试。
    
    这种方法的核心思想与 CAS 相同——不锁定资源，而是基于**版本对比**来实现原子更新。
    
3. **基于版本号的乐观锁**
    
    在表中增加一个 `version` 字段，每次更新时版本号自增：
    
    ```sql
    UPDATE orders
    SET status = 'done', version = version + 1
    WHERE id = 101 AND version = 5;
    
    ```
    
    若更新成功，说明没有并发冲突；若失败，则说明版本已变化，需要重新读取并重试。
    
    这种机制常用于 ORM 框架或微服务架构中，适合中等并发、低冲突的业务场景。
    

# 分布式锁

## 经典的分布式锁

当需要同步的任务分布在不同机器、不同进程之间时，**内存锁**就无能为力了。于是，**分布式锁**登场了。

在工程实践中，最常见的实现方式是使用 **ZooKeeper** 或 **Etcd** 来完成加锁控制。许多现成的库封装得很好，看起来“只需一行代码”就能搞定分布式同步。

模型大致如下：

![image.png](attachment:40843f82-9fe0-4da5-9a50-e4f58a725233:image.png)

看起来模型很棒，成事大吉，进程 A 拿到锁后，可以放心大胆地做一些事情（步骤2），整个过程进程 B 无法获取锁，也无法执行相关的操作，没有并发问题。实际上，如果设计得当，绝大部分情况下这种模型都将运行良好。

**实际情况呢？当然没这么简单。**

如果你继续往下看，会发现——在很多情况下，你“以为安全”的分布式锁，其实并不安全。

## 为啥分布式锁都是错的？

多数分布式锁的底层都是基于 **租约（lease）机制**实现的。问题也正出在这里。租约模型意味着：锁会在超时后自动失效，从而防止进程异常退出时造成永久死锁。但如果持锁方与锁服务之间发生网络分区或延迟，旧的持锁方可能在锁失效后**仍未及时感知**。这就会导致**双主（double ownership）**问题——也就是，同一把锁同时被两个进程持有。

来看一个典型的例子：

![image.png](https://ahan-io.notion.site/image/attachment%3A40843f82-9fe0-4da5-9a50-e4f58a725233%3Aimage.png?table=block&id=29feda9f-236a-8019-9768-cd1cc93ac2f3&spaceId=3841c813-6aff-406c-8c94-6fa3c0018b15&width=1420&userId=&cache=v2)

- 进程 A 获取锁。
- 进程 A 开始执行任务（访问 Service O）。
- A 与锁服务断开连接，锁租约过期。
- B 获取锁并开始执行。
- A 并未感知到断联，继续执行 —— **冲突发生**。

很多人会问：

> “为什么锁失效了，A 不知道？网络不是会报错吗？定时器不会超时吗？”
> 

理论上是的，但现实很残酷。

任何依赖**定时器或心跳机制**的方案，都可能被以下情况破坏：

- CPU 长时间卡顿或调度延迟
- Java 进程 GC 暂停
- 线程死锁
- 内核调度饥饿
- 时钟漂移或系统负载过高

于是你可能遇到这些情况：

1. 锁已经失效，但旧持锁方**完全没感知**。
2. 锁失效，持锁方感知**太迟**。
3. 持锁方知道了，但**没能及时退出**。

**一句话总结：**

> 任何基于租约的分布式锁，都无法严格避免“双主”问题。
> 

Martin Kleppmann 在他的[文章](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)中也指出了这一点。这种分布式锁在工程上是**不严格的（not safe）**

。很多团队误以为“加了锁就安全”，从而在**需要强一致性**的场景中误用这类机制，埋下严重隐患。

当然，这并不意味着分布式锁一无是处。在很多场景下，它依然是一个**实用且简化模型的选择**

。只要业务容忍极少数的并发冲突，这种“弱一致性”的锁机制已经足够好用。当然，你必须要完全清楚这种方案的潜在风险。

## 如何让分布式锁变得“更安全”

Martin Kleppmann 在文中提出了一个重要概念：

> Making the lock safe with fencing
> 

解决思路是：

> 每次客户端成功获取锁时，锁服务返回一个递增的 Fencing Token（防护令牌）。
> 
> 
> 客户端在执行写操作时，必须携带这个令牌，存储层会拒绝令牌编号小于最新值的请求。
> 

这意味着，即使旧的持锁者还在“瞎忙”，它的写请求也会被系统直接拒绝。

常见做法是让存储层维护一个自增计数器或基于 CAS 的版本号机制，用以判定写入的合法性。

**核心思想是：**

> 不去信任应用层的定时器，而让存储层提供最终的防护（Fence）。
> 

只要能达到“旧主写不进去”的效果，这种设计就是安全的。

严格来说，**真正安全的分布式锁，本质上是一种带版本号的访问控制协议，而不是一段租约逻辑。**

# CAS（Compare and Swap）

在现代处理器体系中，几乎所有的 CPU 都提供了一条原子性的 **CAS（Compare and Swap）指令**。顾名思义，它的作用是“比较并交换”。

通常，一个 CAS 操作接收三个参数：当前值 **V**、期望值 **A**、以及准备写入的新值 **B**。其语义可以描述为：

> “我认为当前的值是 A，如果确实如此，那么将它更新为 B；如果不是，则不修改，并告诉我当前真实的值是多少。”
> 

这种机制看似简单，却是无锁并发编程的核心基础之一。CAS 天然具备“冲突检测”的能力，可以优雅地解决多个线程或进程同时修改同一资源时的竞争问题。

## 并发状态更新中的应用

在分布式系统中，我们经常可以用 CAS 的思想来维护对象状态的一致性。比如：

1. “我认为对象当前版本是 **v1**，如果是，请将状态更新为 **s_new**，并把版本号更新为 **v2**；否则，返回失败。”
2. “我认为对象当前状态是 **S1**，如果是，请更新为 **S2**；否则，不修改。”

只要确保版本号是**单调递增**的，就可以保证所有并发的状态变更都按顺序生效，不会发生交叉覆盖。

## ZooKeeper / Etcd 中的 CAS 机制

ZooKeeper 和 Etcd 都内置了类似 CAS 的机制。

以 ZooKeeper 为例，节点的每一次修改都会导致其内部的 **version** 字段自增。

假设现在有如下场景：

- 对象当前版本为 4，状态为 `a`；
- 操作 1 想把状态改为 `b`；
- 操作 2 想把状态改为 `c`；
- 其中，`a→b` 或 `a→c` 都是合法的状态迁移，但 `b→c` 是非法的（例如 a=Running, b=Upgrading, c=Deleting）。

若不使用 CAS 机制，可能会出现以下问题：

1. 操作 1 成功将 `a` 改为 `b`；
2. 操作 2 在旧版本基础上执行，将 `b` 改为 `c`；
3. 系统最终进入非法状态，**对象状态流转错误**。

而使用 CAS 机制后，流程变为：

- **发现冲突：**
    1. 操作 1 与操作 2 都读取到版本 4、状态 a；
    2. 操作 1 提交修改（带版本 4），成功更新为状态 b、版本 5；
    3. 操作 2 提交修改（仍带版本 4），ZooKeeper 拒绝写入（CAS 失败）；
    4. 冲突被发现，对象状态保持正确。
- **没有冲突：**
    1. 操作 1 读取版本 4、状态 a；
    2. 操作 1 更新成功，状态变为 b、版本为 5；
    3. 操作 2 再次读取时，看到版本已变为 5；
    4. 检查发现不允许从 b→c，主动放弃修改。
        
        **对象状态流转保持一致。**
        

因此，CAS 可以确保并发修改在逻辑上是**线性化（Linearizable）**的，每一次状态变更都能看到上一次的结果，从而保证状态机的正确推进。

> ZooKeeper、Etcd 之所以可以提供 CAS 的语义，本质上是因为他们底层实现了共识算法。我们在接下来的内容中会介绍共识算法。
> 

### 实践建议

在工程实践中，**CAS 的操作对象应与最终存储位置保持一致**。

例如，如果你使用 ZooKeeper 存储对象状态，那么最好直接利用 ZooKeeper 的版本号进行 CAS 操作，而不要在应用层额外维护一个“伪版本号”，以免出现不同步的情况。

在实际系统中，CAS 往往会与其他同步手段配合使用。

一种典型的方式是：

- 利用 ZooKeeper 的分布式锁，选出一个 **leader** 节点，由它负责执行大多数运维操作；
- 然后在 leader 执行状态修改时，配合使用 **CAS**，从而在出现“双主”场景时及时发现并阻止冲突。

这种组合方式让系统既能在大多数情况下保持简洁高效，又能在极端条件下做到**自我纠错与防护**，从而提高整个系统的稳定性与一致性。

# 从并发到共识

在单机程序中，我们用锁、信号量或 CAS 操作来协调并发；

但当系统被拆分到多台机器上，这些手段就失去了作用。

在多节点的世界里，没有谁能天然地持有“全局的锁”——

我们必须回答更根本的问题：

**“谁来决定当前的状态？”**

**“哪一次写入才算真正生效？”**

当计算跨越机器边界，问题从“并发控制”演变为“共识达成”。

此时，锁的粒度不再是线程，而是节点。

我们需要的不再是“互斥”，而是一种能让多个节点对事件顺序达成一致的机制——

**分布式一致性协议（Consensus Protocol）**。

它是分布式系统的“底层秩序”，决定了在网络分裂、节点宕机、消息延迟甚至误判的情况下，系统如何维持统一的真相。

分布式一致性协议的核心目标，是回答这样一个问题：

> 当不同节点看到的世界不同时，我们如何决定“哪个版本才是真的”？
> 

围绕这个问题，分布式系统的发展史，几乎就是一致性协议的演进史。

**Paxos：从理论起点出发**

**Paxos** 由 **Leslie Lamport** 于 1990 年提出，是现代分布式一致性理论的基石。

它通过三种角色——**提议者（Proposer）**、**接受者（Acceptor）**、**学习者（Learner）**——定义了一个确保唯一“共识值”的过程。

即使部分节点失效，系统仍能正确地达成一致。

它证明了共识是可行的，只是代价高昂。

Paxos 的设计精密而抽象，理论完美却难以直接落地。

理解 Paxos，就等于理解了分布式的复杂性：

“光是让大家意见一致，就已经足够难了。”

**Raft：让共识可被实现**

**Raft** 协议的诞生，是工程理性对复杂性的回应。

它并未推翻 Paxos，而是将其复杂的逻辑结构化、可讲述化——

拆解为三个核心阶段：

**选主（Leader Election）**、**日志复制（Log Replication）**、**安全提交（Commit）**。

Raft 的最大贡献，不在于性能，而在于“可理解性”。

它让共识的机制不再晦涩难懂，工程师只需数小时即可掌握要点。

如今，Raft 已成为工业标准：

Etcd、Consul、TiKV 等核心系统都以它为基础。

Raft 让一致性从学术概念，变成了工程实践。

**CRAQ：确定性的工程实现**

随着系统规模不断扩大，一致性协议也开始在性能与正确性之间寻找平衡。**CRAQ（Chain Replication with Active Queries）** 是这种思考的产物。

它采用**链式复制（Chain Replication）**架构：

- **写操作** 从链头顺序传递到链尾，由尾节点确认后才算提交；
- **读操作** 可在任意节点执行，但若数据尚未完全提交，会自动回退至链尾，确保读取到的始终是**线性化（Linearizable）**状态。

在 **DeepSeek 的 3FS 文件系统**中，CRAQ 被用作核心一致性协议。它让整个存储系统表现得像一个**单一、原子化的存储体**：写操作严格串行，提交后立即可见；每一次读取，都能获得全局最新的状态。

在 Raft 中，为了保持线性一致性，通常只能从 Leader 读取（Follower 的数据可能滞后）。而 CRAQ 的 Active Queries 机制允许读请求在任意节点上执行，**只在需要时回退到链尾节点**。这使得在读多写少的场景中，CRAQ 的读吞吐可达到数倍提升。